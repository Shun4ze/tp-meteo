{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions d'Installation et de Configuration de l'Environnement pour le Projet Kafka\n",
    "\n",
    "Ce document fournit un guide pour installer Python 3.10, configurer un environnement virtuel, et installer les bibliothèques nécessaires pour le projet de streaming de données avec Kafka.\n",
    "\n",
    "---\n",
    "\n",
    "## Installation de Python 3.10\n",
    "\n",
    "1. **Ajouter le dépôt de Python** :\n",
    "\n",
    "   ```bash\n",
    "   sudo apt update\n",
    "   sudo apt install -y software-properties-common\n",
    "   sudo add-apt-repository ppa:deadsnakes/ppa\n",
    "   sudo apt update\n",
    "\n",
    "\n",
    "2. **Installer Python 3.10 :** :\n",
    "\n",
    "   ```bash\n",
    "   sudo apt install python3.10\n",
    "\n",
    "3. **Installer le module venv pour Python 3.10 :** :\n",
    "\n",
    "   ```bash\n",
    "   sudo apt install python3.10-venv\n",
    "\n",
    "\n",
    "## Création et Activation d'un Environnement Virtuel\n",
    "\n",
    "4. **Créer l’environnement virtuel :** :\n",
    "\n",
    "   ```bash\n",
    "   python3.10 -m venv kafka_env\n",
    "\n",
    "5. **Activer l’environnement virtuel :** :\n",
    "\n",
    "   ```bash\n",
    "   source kafka_env/bin/activate\n",
    "\n",
    "## Installation des Bibliothèques Requises\n",
    "Dans l’environnement virtuel, installez les bibliothèques suivantes :\n",
    "\n",
    "5. **kafka-python : pour interagir avec Kafka en tant que producteur et consumer.** :\n",
    "\n",
    "   ```bash\n",
    "   pip install kafka-python\n",
    "5. **requests : pour récupérer les données depuis l'API** :\n",
    "\n",
    "   ```bash\n",
    "   pip install requests\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coord': {'lon': 2.3488, 'lat': 48.8534},\n",
       " 'weather': [{'id': 803,\n",
       "   'main': 'Clouds',\n",
       "   'description': 'broken clouds',\n",
       "   'icon': '04d'}],\n",
       " 'base': 'stations',\n",
       " 'main': {'temp': 12.66,\n",
       "  'feels_like': 11.89,\n",
       "  'temp_min': 11.88,\n",
       "  'temp_max': 13.31,\n",
       "  'pressure': 1029,\n",
       "  'humidity': 73,\n",
       "  'sea_level': 1029,\n",
       "  'grnd_level': 1019},\n",
       " 'visibility': 10000,\n",
       " 'wind': {'speed': 3.09, 'deg': 10},\n",
       " 'clouds': {'all': 75},\n",
       " 'dt': 1731588447,\n",
       " 'sys': {'type': 2,\n",
       "  'id': 2012208,\n",
       "  'country': 'FR',\n",
       "  'sunrise': 1731567517,\n",
       "  'sunset': 1731600706},\n",
       " 'timezone': 3600,\n",
       " 'id': 2988507,\n",
       " 'name': 'Paris',\n",
       " 'cod': 200}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Configuration\n",
    "API_KEY = '6365575828f48fe0263c2d56aa2e323c'  # Clé API\n",
    "CITIES = ['Paris']  # Villes cibles\n",
    "\n",
    "\n",
    "# Fonction pour récupérer les données météo\n",
    "def get_weather_data(city):\n",
    "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={API_KEY}&units=metric\"\n",
    "    response = requests.get(url)\n",
    "    return response.json() if response.status_code == 200 else None\n",
    "\n",
    "get_weather_data(\"Paris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo apt install python3.10-venv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kafka.producer.future.FutureRecordMetadata at 0x774bf204c8b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple pour collecter des données dans un topic Kafka\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "KAFKA_SERVER = 'localhost:9092'\n",
    "\n",
    "# Initialisation du producteur Kafka\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[KAFKA_SERVER],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "data =  {'temp': 11.32,\n",
    "  'feels_like': 10.67,\n",
    "  'temp_min': 10.21,\n",
    "  'temp_max': 12.36,\n",
    "  'pressure': 1030,\n",
    "  'humidity': 83,\n",
    "  'sea_level': 1030,\n",
    "  'grnd_level': 1020}\n",
    "\n",
    "# Envoi des données en continu\n",
    "\n",
    "producer.send(\"tp-meteo\",  value=data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'temp': 11.32, 'feels_like': 10.67, 'temp_min': 10.21, 'temp_max': 12.36, 'pressure': 1030, 'humidity': 83, 'sea_level': 1030, 'grnd_level': 1020}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m consumer \u001b[38;5;241m=\u001b[39m KafkaConsumer(\n\u001b[1;32m      9\u001b[0m     KAFKA_TOPIC,\n\u001b[1;32m     10\u001b[0m     bootstrap_servers\u001b[38;5;241m=\u001b[39m[KAFKA_SERVER],\n\u001b[1;32m     11\u001b[0m     auto_offset_reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearliest\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m     value_deserializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: json\u001b[38;5;241m.\u001b[39mloads(x\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Consommer et afficher les messages\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m consumer:\n\u001b[1;32m     17\u001b[0m     data \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mvalue       \n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data)\n",
      "File \u001b[0;32m/workspaces/tp-meteo/kafka_env/lib/python3.10/site-packages/kafka/consumer/group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v1()\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/tp-meteo/kafka_env/lib/python3.10/site-packages/kafka/consumer/group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/tp-meteo/kafka_env/lib/python3.10/site-packages/kafka/consumer/group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1115\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[0;32m-> 1116\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[1;32m   1122\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/tp-meteo/kafka_env/lib/python3.10/site-packages/kafka/consumer/group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    653\u001b[0m remaining \u001b[38;5;241m=\u001b[39m timeout_ms\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[1;32m    657\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m/workspaces/tp-meteo/kafka_env/lib/python3.10/site-packages/kafka/consumer/group.py:702\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpoll(timeout_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    701\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout_ms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mtime_to_next_poll() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m--> 702\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;66;03m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mneed_rejoin():\n",
      "File \u001b[0;32m/workspaces/tp-meteo/kafka_env/lib/python3.10/site-packages/kafka/client_async.py:602\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    599\u001b[0m             timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretry_backoff_ms\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    600\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, timeout)  \u001b[38;5;66;03m# avoid negative timeouts\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# if handlers need to acquire locks\u001b[39;00m\n\u001b[1;32m    606\u001b[0m responses\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[0;32m/workspaces/tp-meteo/kafka_env/lib/python3.10/site-packages/kafka/client_async.py:634\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_send_sockets()\n\u001b[1;32m    633\u001b[0m start_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 634\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m end_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sensors:\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:469\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "# Configuration de Kafka\n",
    "KAFKA_TOPIC = 'tp-meteo'\n",
    "KAFKA_SERVER = 'localhost:9092'\n",
    "\n",
    "# Initialisation du consumer Kafka\n",
    "consumer = KafkaConsumer(\n",
    "    KAFKA_TOPIC,\n",
    "    bootstrap_servers=[KAFKA_SERVER],\n",
    "    auto_offset_reset='earliest',\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "# Consommer et afficher les messages\n",
    "for message in consumer:\n",
    "    data = message.value       \n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Pipeline de Données Météorologiques en Temps Réel avec Kafka\n",
    "\n",
    "**Objectif** : Dans ce TP, vous allez utiliser Kafka pour transmettre des données météorologiques en temps réel d’un producteur (qui récupère les données) vers un consumer (qui les affiche). Vous configurerez un producteur pour récupérer les données météo de plusieurs villes via une API, puis un consumer pour consommer et afficher ces données en continu.\n",
    "\n",
    "---\n",
    "\n",
    "## Partie 1 : Producteur Kafka – Récupération et Envoi des Données Météorologiques\n",
    "\n",
    "Le producteur Kafka va récupérer les données météo pour trois villes cibles et les envoyer dans un topic Kafka nommé `tp-meteo`. \n",
    "\n",
    "### Étapes :\n",
    "\n",
    "1. **Configuration de l'API et des Villes** :\n",
    "   - Utilisez une clé API pour accéder à OpenWeatherMap (une API de données météo).\n",
    "   - Définissez trois villes cibles : `Paris`, `London`, et `Tokyo`.\n",
    "\n",
    "2. **Initialisation du Producteur Kafka** :\n",
    "   - Configurez Kafka pour qu’il envoie des messages au serveur `localhost:9092` dans le topic `tp-meteo`.\n",
    "   - Kafka sera utilisé pour envoyer les données météorologiques formatées en JSON.\n",
    "\n",
    "3. **Fonction de Récupération des Données** :\n",
    "   - Écrivez une fonction `get_weather_data` qui récupère les informations météo pour une ville donnée.\n",
    "   - La fonction envoie une requête à l’API et retourne les données si la requête est réussie.\n",
    "\n",
    "4. **Boucle d’Envoi en Continu** :\n",
    "   - Utilisez une boucle pour envoyer les données de chaque ville au topic Kafka toutes les minutes.\n",
    "   - Affichez un message de confirmation dans la console pour chaque envoi réussi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KafkaProducer' from 'kafka' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaProducer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'KafkaProducer' from 'kafka' (unknown location)"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import requests\n",
    "import json\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"6365575828f48fe0263c2d56aa2e323c\"  \n",
    "\n",
    "cities = [\"Paris\", \"London\", \"Tokyo\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_data(city):\n",
    "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={API_KEY}&units=metric\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200: #200 veut dire que la requete a reussit\n",
    "        return response.json()  # Retourne les données sous forme de dictionnaire\n",
    "    else:\n",
    "        print(f\"Erreur lors de la récupération des données pour {city}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KafkaProducer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m producer \u001b[38;5;241m=\u001b[39m \u001b[43mKafkaProducer\u001b[49m(\n\u001b[1;32m      2\u001b[0m     bootstrap_servers\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost:9092\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Connexion à Kafka\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     value_serializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m v: json\u001b[38;5;241m.\u001b[39mdumps(v)\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Sérialisation des données en JSON\u001b[39;00m\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KafkaProducer' is not defined"
     ]
    }
   ],
   "source": [
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',  # Connexion à Kafka\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')  # Sérialisation des données en JSON\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Lancer l'envoi des données\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43msend_weather_data_to_kafka\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m, in \u001b[0;36msend_weather_data_to_kafka\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_weather_data_to_kafka\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m city \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcities\u001b[49m:\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;66;03m# Récupérer les données météo\u001b[39;00m\n\u001b[1;32m      5\u001b[0m             weather_data \u001b[38;5;241m=\u001b[39m get_weather_data(city)\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m weather_data:\n\u001b[1;32m      7\u001b[0m                 \u001b[38;5;66;03m# Envoi des données dans le topic Kafka\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cities' is not defined"
     ]
    }
   ],
   "source": [
    "def send_weather_data_to_kafka():\n",
    "    while True:\n",
    "        for city in cities:\n",
    "            # Récupérer les données météo\n",
    "            weather_data = get_weather_data(city)\n",
    "            if weather_data:\n",
    "                # Envoi des données dans le topic Kafka\n",
    "                producer.send('tp-meteo', value=weather_data)\n",
    "                print(f\"Données météo envoyées pour {city}: {weather_data['main']['temp']}°C\")\n",
    "        \n",
    "        # Attendre 1 minute avant de renvoyer les données\n",
    "        time.sleep(60)\n",
    "\n",
    "# Lancer l'envoi des données\n",
    "send_weather_data_to_kafka()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
